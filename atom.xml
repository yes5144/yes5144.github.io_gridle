<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://yes5144.github.io/</id>
    <title>Talk is cheap. Show me the code</title>
    <updated>2020-03-16T15:49:45.999Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://yes5144.github.io/"/>
    <link rel="self" href="https://yes5144.github.io//atom.xml"/>
    <subtitle>你第十年的目标是CTO，你打算如何实现？</subtitle>
    <logo>https://yes5144.github.io//images/avatar.png</logo>
    <icon>https://yes5144.github.io//favicon.ico</icon>
    <rights>All rights reserved 2020, Talk is cheap. Show me the code</rights>
    <entry>
        <title type="html"><![CDATA[golang 前置补0]]></title>
        <id>https://yes5144.github.io//post/golang-qian-zhi-bu-0</id>
        <link href="https://yes5144.github.io//post/golang-qian-zhi-bu-0">
        </link>
        <updated>2020-03-16T15:23:51.000Z</updated>
        <content type="html"><![CDATA[<pre><code>// https://www.cnblogs.com/forrestsun/p/3666069.html
package main

import (
    &quot;fmt&quot;
)

func main() {
    a := 1
    fmt.Println(a)
    //前置补0
    fmt.Printf(&quot;%03d&quot;, a)
    fmt.Println(&quot;&quot;)
    fmt.Printf(&quot;%0*d&quot;, 3, a)
}

	// proName := []string{&quot;Coder&quot;, &quot;鹿鼎记&quot;, &quot;天龙八部&quot;, &quot;连城诀&quot;}
	// appName := []string{&quot;golang&quot;, &quot;python&quot;, &quot;虚竹&quot;, &quot;段誉&quot;, &quot;水生&quot;, &quot;韦小宝&quot;, &quot;狄云&quot;, &quot;丁典&quot;}
	// log.Println(proName[rand.Intn(len(proName))])
	// log.Println(appName[rand.Intn(len(appName))])
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mysql 日期处理]]></title>
        <id>https://yes5144.github.io//post/mysql-ri-qi-chu-li</id>
        <link href="https://yes5144.github.io//post/mysql-ri-qi-chu-li">
        </link>
        <updated>2019-12-01T11:07:46.000Z</updated>
        <content type="html"><![CDATA[<pre><code>https://www.jb51.net/article/132425.htm

## 1、当前日期
select DATE_SUB(curdate(),INTERVAL 0 DAY) ;
## 2、明天日期
select DATE_SUB(curdate(),INTERVAL -1 DAY) ;
## 3、昨天日期
select DATE_SUB(curdate(),INTERVAL 1 DAY) ;
## 4、前一个小时时间
select date_sub(now(), interval 1 hour);
## 5、后一个小时时间
select date_sub(now(), interval -1 hour);
## 6、前30分钟时间
select date_add(now(),interval -30 minute)
## 7、后30分钟时间
select date_add(now(),interval 30 minute)
## 8、取得当天：
SELECT curdate();
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[TheadPool片段]]></title>
        <id>https://yes5144.github.io//post/theadpool-pian-duan</id>
        <link href="https://yes5144.github.io//post/theadpool-pian-duan">
        </link>
        <updated>2019-11-18T15:30:08.000Z</updated>
        <content type="html"><![CDATA[<pre><code>def foo():
    import time
    import datetime
    print(datetime.datetime.now())
    time.sleep(1)

from concurrent.futures import ThreadPoolExecutor
pool = ThreadPoolExecutor(2)
for i in range(10):
    pool.submit(foo)

pool.shutdown(wait=True)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[k8s 实战]]></title>
        <id>https://yes5144.github.io//post/k8s-shi-zhan</id>
        <link href="https://yes5144.github.io//post/k8s-shi-zhan">
        </link>
        <updated>2019-11-01T02:56:59.000Z</updated>
        <content type="html"><![CDATA[<h3 id="k8s集群一键部署">k8s集群一键部署</h3>
<p>https://www.kubernetes.org.cn/5462.html</p>
<h3 id="k8sui展示">k8sUI展示</h3>
<h3 id="k8s集成ceph分布式存储">k8s集成ceph分布式存储</h3>
<h3 id="k8s服务部署">k8s服务部署</h3>
<h3 id="k8s集群监控">k8s集群监控</h3>
<h3 id="k8s日志监控">k8s日志监控</h3>
<h3 id="k8s常用命令">k8s常用命令</h3>
<pre><code>## kubectl命令行工具设置别名和tab补全

## 列出集群中节点、pod、服务和ReplicationController

##
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linux个性化提示符]]></title>
        <id>https://yes5144.github.io//post/linux-ge-xing-hua-ti-shi-fu</id>
        <link href="https://yes5144.github.io//post/linux-ge-xing-hua-ti-shi-fu">
        </link>
        <updated>2019-10-31T12:52:17.000Z</updated>
        <content type="html"><![CDATA[<pre><code>## /etc/profile 添加
#PS1='\[\e[31;40m\][\u@\h \W]\$ \[\e[0m\]'
export PS1=&quot;\[$(tput bold)$(tput setab 0)$(tput setaf 1)\][\u@\h \W] # \[$(tput sgr0)\]&quot;

</code></pre>
<h3 id="自定义命令审查">自定义命令审查</h3>
<pre><code>## /etc/profile 添加
#export PROMPT_COMMAND='logger -p local0.info &quot;$(ifconfig | grep -E &quot;eth|em&quot; -A 1 | grep &quot;172.2&quot; | grep -oP &quot;(?&lt;=inet )[\d\.]+&quot;) $(who am i |awk &quot;{print \$1\&quot; \&quot;\$2\&quot; \&quot;\$3\&quot; \&quot;\$4\&quot; \&quot;\$5}&quot;) [`pwd`] $(history1 | { read x cmd; echo &quot;$cmd&quot;; })&quot;'
export PROMPT_COMMAND='logger -p local0.info &quot;$(hostname -i) $(who am i |awk &quot;{print \$1\&quot; \&quot;\$2\&quot; \&quot;\$3\&quot; \&quot;\$4\&quot; \&quot;\$5}&quot;) [`pwd`] $(history 1 | { read x cmd; echo &quot;$cmd&quot;; })&quot;'

## /etc/rsyslog.conf 添加
local0.*                                                /var/log/bash.log

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[邮箱配置]]></title>
        <id>https://yes5144.github.io//post/you-xiang-pei-zhi</id>
        <link href="https://yes5144.github.io//post/you-xiang-pei-zhi">
        </link>
        <updated>2019-10-29T14:46:20.000Z</updated>
        <content type="html"><![CDATA[<pre><code>## qq企业邮箱
smtp.exmail.qq.com
name@company.com
password

## 163.com
smtp.163.com
name@163.com
password

## qq.com
smtp.exmail.qq.com
端口625
name@qq.com
动态口令
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[指定pip源 pip  install]]></title>
        <id>https://yes5144.github.io//post/zhi-ding-pip-yuan-pip-install</id>
        <link href="https://yes5144.github.io//post/zhi-ding-pip-yuan-pip-install">
        </link>
        <updated>2019-10-29T14:42:33.000Z</updated>
        <content type="html"><![CDATA[<pre><code>pip install -r requirements.txt  -i http://pypi.douban.com/simple --trusted-host pypi.douban.com
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Django 眼高手低001]]></title>
        <id>https://yes5144.github.io//post/django-yan-gao-shou-di-001</id>
        <link href="https://yes5144.github.io//post/django-yan-gao-shou-di-001">
        </link>
        <updated>2019-10-22T14:21:44.000Z</updated>
        <content type="html"><![CDATA[<h2 id="django创建model后">Django创建model后</h2>
<p>model.table.objects.all()
model.table.objects.filter(id=1)
QuerySet 可迭代</p>
<p>model.table.objects.get(id=1)
不可迭代</p>
<p>https://www.django.cn/course/show-31.html</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ELK日志监控]]></title>
        <id>https://yes5144.github.io//post/elk-ri-zhi-jian-kong</id>
        <link href="https://yes5144.github.io//post/elk-ri-zhi-jian-kong">
        </link>
        <updated>2019-10-16T16:31:02.000Z</updated>
        <content type="html"><![CDATA[<p>Fluentd 主要负责日志采集，其他开源组件还有Filebeat、Flume、Fluent Bit等，也有一些应用采集Log4g等日志组件直接输出日志。
Kafka 主要负责数据整流合并，避免突发日志流量直接冲击Logstash，业内也有用Redis的方案。
Logstash 负责日志整理，可以完成日志过滤、日志修改等功能。
Elasticsearch 负责日志储存和日志检索，自带分布式存储，可以将采集的日志进行分片存储。为保证数据高可用，Elasticsearch引入了多副本概念，并通过Lucene实现日志的索引和查询。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[k8s-002-重要术语]]></title>
        <id>https://yes5144.github.io//post/k8s-002-zhong-yao-zhu-yu</id>
        <link href="https://yes5144.github.io//post/k8s-002-zhong-yao-zhu-yu">
        </link>
        <updated>2019-09-21T02:16:57.000Z</updated>
        <content type="html"><![CDATA[<h2 id="pod是容器的集合通常会将紧密相关的一组容器放到一个pod中同一个pod中的所有容器共享ip地址和port空间也就是说它们在一个network-namespace中">pod是容器的集合，通常会将紧密相关的一组容器放到一个Pod中，同一个Pod中的所有容器共享IP地址和Port空间，也就是说它们在一个network namespace中。</h2>
<h2 id="pod是kubernetes调度的最小单位同一pod中的容器始终被一起调度">Pod是kubernetes调度的最小单位，同一Pod中的容器始终被一起调度。</h2>
<p>kubectl get nodes
kubectl cluster-info</p>
<p>kubectl run kubernetes-bootcamp --image=docker.io/jocatalin/kubernetes-bootcamp:v1  --port=8080</p>
<p>kubectl get pods
kubectl get pod  --all-namespaces
kubectl describe pod <Pod Name> --namespaces=kube-system</p>
<p>kubectl expose deployment/kubernetes-bootcamp --type=&quot;NodePort&quot;  --port 8080</p>
<p>kubectl get services
kubectl get deployments</p>
<h2 id="弹性伸缩">弹性伸缩</h2>
<h3 id="scale-up-3个副本">scale up 3个副本</h3>
<p>kubectl scale deployment/kubernetes-bootcamp  --replicas=3</p>
<p>kubectl get deployments
kubectl get pods</p>
<h3 id="scale-down">scale down</h3>
<p>kubectl scale deployment/kubernetes-bootcamp  --replicas=2</p>
<p>kubectl get deployments
kubectl get pods</p>
<h2 id="滚动更新">滚动更新</h2>
<p>kubectl set image deployment/kubernetes-bootcamp  kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2</p>
<p>kubectl get pods
kubectl get pods
kubectl get pods
kubectl get pod -o wide</p>
<h2 id="版本回退">版本回退</h2>
<p>kubectl rollout undo deployment/kubernetes-bootcamp</p>
<p>kubectl get pods</p>
<p>Cluster
Master
Node
Pod</p>
<p>Controller
Service
Namespace</p>
<p>Master 是Kubernetes Cluster的大脑，
运行着的 Deamon服务包括 kube-apiserver, kube-scheduler, kube-controller-manager, etcd, Pod网络（例如Flannel）</p>
<p>Node 是Pod运行的地方，Kubernets支持Docker，rkt等容器
运行 kubelet, kube-proxy和Pod网络（如flannel）</p>
<p>kubectl run httpd-app --image=httpd --replicas=2
kubectl get deployment
kubectl get pod -o wide</p>
<h2 id="然后kubernetes-部署了-deployment-httpd-app-有两个副本的pod分别在k8s-node1和k8s-node2">然后kubernetes 部署了 deployment httpd-app, 有两个副本的Pod，分别在k8s-node1和k8s-node2</h2>
<h2 id="详细讨论整个部署过程">详细讨论整个部署过程，</h2>
<p>1, kubectl 发送部署请求到API Server
2, API Server 通知 Controller Manager 创建一个deployment资源
3, Scheduler 执行调度任务，将两个副本Pod分发到 k8s-node1和k8s-node2
4, k8s-node1和k8s-node2 的kubectl 在各自的节点上创建并运行Pod</p>
<h2 id="命令-vs-配置文件">命令 vs 配置文件</h2>
<p>kubectl run nginx-deployment --image=nginx:1.7.9 --replicas=2</p>
<p>cat nginx.yml</p>
<pre><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 2
  template:
    metadata:
	labels:
	  app: web_server
  spec:
    containers:
	- name: nginx
	  image: nginx:1.7.9
</code></pre>
<p>kubectl apply -f nginx.yml</p>
<p>kubectl create/replace/edit/patch</p>
<h2 id="failover">Failover</h2>
<h3 id="模拟故障k8s-node2直接关机node2">模拟故障k8s-node2，直接关机node2</h3>
<h3 id="等一段时间k8s-检查到-node2不可用将node2标记为-unknown并在node1上创建两个pod维持总副本数为3">等一段时间，k8s 检查到 node2不可用，将node2标记为 Unknown，并在node1上创建两个Pod，维持总副本数为3，</h3>
<h3 id="当node2-恢复后unknown的pod会被删除不过已经运行的pod不会重新调度会-node2">当node2 恢复后，Unknown的Pod会被删除，不过已经运行的Pod不会重新调度会 node2</h3>
<h2 id="删除-nginx-deployment">删除 nginx-deployment</h2>
<p>kubectl delete deployment nginx-deployment</p>
<h2 id="为node添加label">为Node添加label</h2>
<p>kubectl label node node k8s-node1 disktype=ssd</p>
<p>kubectl get node --show-labels</p>
<h2 id="删除label-disktype">删除label disktype</h2>
<p>kubectl label node k8s-node1 disktype-</p>
<h2 id="daemonset">DaemonSet</h2>
<p>Deployment 部署的副本Pod会分布在各个node，每个node都可能运行几个副本
DaemonSet 的不同之处：每个node最多只能运行一个副本</p>
<p>DaemonSet 的典型应用场景：
1, 在集群的每个节点上运行存储 Daemon，比如 glusterd 或 ceph
2, 在每个节点上运行日志收集 Daemon，比如 flunented 或 logstash
3，在每个节点上运行监控 Daemon，比如 Prometheus Node Exporter 或 collectd</p>
<p>kubectl get daemonset --namespaces=kube-system</p>
<p>wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</p>
<p>kubectl edit daemonset kube-proxy --namespaces=kube-system</p>
<h2 id="部署自己的-daemonset-prometheus-node-exporter">部署自己的 DaemonSet--Prometheus- Node Exporter</h2>
<p>cat node_exporter.yml</p>
<pre><code>apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: node-exporter-daemonset
spec:
  template:
    metadata:
	  labels:
	    app: prometheus
	spec:
	  hostNetwork: true
	  containers:
	  - name: node-exporter
	    image: prom/node-exporter
		imagePullPolicy: IfNotPresent
		command:
		- /bin/node_exporter
		- --path.procfs
		- /host/proc
		- --path.sysfs
		- /host/sys
		- --collector.filesystem.ignored-mount-points
		- ^/(sys|proc|dev|host|etc)($|/)
		volumMounts:
		- name: proc
		  mountPath: /host/proc
		- name: sys
		  mountPath: /host/sys
		- name: root
		  mountPath: /rootfs
	  volumes:
	  - name: proc
	    hostPath:
		  path: /proc
	  - name: sys
	    hostPath:
		  path: /sys
	  - name: root
	    hostPath:
		  path: /
</code></pre>
<p>kubectl apply -f node_exporter.yml</p>
<p>kubectl get pod -o wide</p>
<h2 id="job">Job</h2>
<p>容器安装持续运行的时间可以分为两类：服务类容器和 工作类容器</p>
<p>k8s 的 Deployment、ReplicaSet、DaemonSet 都用于管理服务类容器；
对于工作类容器，我们使用job</p>
<p>下面是简单的Job 配置文件 myjob.yml
cat myjob.yml</p>
<pre><code>apiVersion: batch/v1
kind: Job
metadata:
  name: myjob
spec:
  template:
    metadata:
	  name: myjob
	spec:
	  containers:
	  - name: hello
	    image: busybox
		command: [&quot;echo&quot;, &quot;hello k8s job&quot;]
	  restartPolicy: Never
</code></pre>
<p>kubectl apply -f myjob.yml
kubectl get job
kubectl get pod</p>
<p>kubectl get pod --show-all</p>
<p>kubectl logs myjob-xxx</p>
<p>kubectl delete -f myjob.yml</p>
<p>kubectl get namespace</p>
<h2 id="kube-public-部署service-httpd2-svc">kube-public 部署Service Httpd2-svc</h2>
<p>cat httpd2.yml</p>
<pre><code>apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: httpd2
  namespace: kube-public
spec:
  replicas: 3
  template:
    metadata:
	  label:
	    run: httpd2
	spec: 
	  containers:
	  - name: httpd2
	    image: httpd
		ports:
		- containerPort: 80
		
---
apiVersion: v1
kind: Service
metadata:
  name: httpd2-svc
  namespace: kube-public
spec:
  selector:
    run: httpd2
  ports:
  - protocol: TCP
    port: 8080
	targetPort: 80
</code></pre>
<p>kubectl apply -f httpd2.yml
kubectl get service --namespace=kube-public</p>
<h2 id="外网如何访问service">外网如何访问service</h2>
<p>ClusterIP
NodePort
LoadBalancer</p>
<h2 id="rolling-update">Rolling Update</h2>
<p>kubectl apply -f httpd.v1.yml  --record
kubectl apply -f httpd.v2.yml  --record
kubectl apply -f httpd.v3.yml  --record</p>
<p>kubectl rollout history deployment httpd</p>
<p>kubectl rollout undo deployment httpd --to-revision=1</p>
<p>kubectl get deployment httpd -o wide</p>
<h2 id="数据管理">数据管理</h2>
<p>k8s volume支持多种backend 类型，包括 emptyDir, hostPath, GCE Persistent Disk, AWS Elastic Block Store, NFS, Ceph等</p>
<h2 id="监控">监控</h2>
<h2 id="ui">UI</h2>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Docker入门命令]]></title>
        <id>https://yes5144.github.io//post/docker-ru-men-ming-ling</id>
        <link href="https://yes5144.github.io//post/docker-ru-men-ming-ling">
        </link>
        <updated>2019-08-26T15:22:52.000Z</updated>
        <content type="html"><![CDATA[<h3 id="docker-基础命令">Docker 基础命令</h3>
<pre><code>docker pull
docker search 
docker push
docker images
docker ps
docker create -it ubuntu:latest
docker start|stop|restart

docker run ubuntu /bin/echo 'Hello world'

docker run -it ubuntu:14.04  /bin/bash
docker ps
docker logs ID_or_name
## 列出所有容器的ID
docker ps -qa

docker exec -it id_or_name /bin/bash

docker rm id_or_name

## 导入容器
docker  export  -o test_name.tar   id_or_name
ls
docker  export id_or_name &gt; test_name.tar22
ls
## 导出容器
docker  import  test_name_tar  -  test/ubuntu:v1.0
</code></pre>
<h3 id="docker-数据管理">Docker 数据管理</h3>
<blockquote>
<p>数据卷：容器内数据直接映射到本地主机环境
数据库容器：使用特定容器维护数据卷</p>
</blockquote>
<pre><code>## 在容器内创建一个数据卷
docker run -d -P  --name web  -v /webapp  training/webapp  python app.py

docker run  -d -P --name web  -v /src/webapp:/opt/webapp  training/webapp  python  app.py

## 数据卷容器
docker run -it -v /dbdata  --name dbdata ubuntu
docker run -it --volumes-from  dbdata  --name db1 ubuntu
docker run -it --volumes-from  dbdata  --name db2 ubuntu
## 也可以从已经挂载了容器卷的容器来挂载数据卷
docker run -d --volumes-from  db1  --name db1 training/postgres

## 数据卷容器的备份
docker run --volumes-from dbdata -v ${pwd}:/backup  --name worker ubuntu  tar cvf /backup/backup.tar /dbdata

## 数据卷容器恢复
### 首先创建一个带有数据卷的容器dbdata2
docker  run -v  /dbdata  --name dbdata2 ubuntu  /bin/bash
docker  run --volumes-from dbdata2 -v ${pwd}:/backup busybox tar xvf /backup/backup.tar
</code></pre>
<h3 id="端口映射和容器互联">端口映射和容器互联</h3>
<pre><code>## -P Docker会随机映射一个49000-49900端口到容器开放的端口
## -p Host_port:Container_port指定端口

docker run -d -P training/webapp python app.py

docker logs -f Name_or_ID

docker port  Name_or_ID Port

docker inspect Name_or_ID

## {} makedown 默认解析了，请在使用时自行删除反斜杠
docker inspect -f &quot;\{\{ .Name \}\}&quot; Name_or_ID
## 容器互联 --link
docker run -d --name db training/postgres

docker run -d  -P --name web --link db:db training/webapp python app.py
## --link name:alias; 其中name是连接的容器名称，alias是这个连接的别名
docker run --rm --name web2 --link db:db training/webapp env
## 父容器是什么容器？？
## 除了环境变量之外，Docker还添加host信息到父容器的/etc/hosts文件。下面是父容器webd hosts文件
docker run -it --rm --link db:db training/webapp /bin/bash

root@lkdje323k3: # cat /etc/hosts
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[二进制部署k8s]]></title>
        <id>https://yes5144.github.io//post/er-jin-zhi-bu-shu-k8s</id>
        <link href="https://yes5144.github.io//post/er-jin-zhi-bu-shu-k8s">
        </link>
        <updated>2019-08-18T09:35:17.000Z</updated>
        <content type="html"><![CDATA[<h3 id=""></h3>
<h3 id="部署etcd">部署etcd</h3>
<pre><code>
## 生成自签证书
curl -L https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o /usr/local/bin/cfssl
curl -L https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o /usr/local/bin/cfssljson
curl -L https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -o /usr/local/bin/cfssl-certinfo
chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson /usr/local/bin/cfssl-certinfo

## 创建两个证书存放目录
mkdir -p  /root/k8s/{etcd-cert,k8s-cert}
## 生成认证机构
cd  /root/k8s/etcd-cert
cat &gt; ca-config.json &lt;&lt;EOF
{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;87600h&quot;
    },
    &quot;profiles&quot;: {
      &quot;www&quot;: {
         &quot;expiry&quot;: &quot;87600h&quot;,
         &quot;usages&quot;: [
            &quot;signing&quot;,
            &quot;key encipherment&quot;,
            &quot;server auth&quot;,
            &quot;client auth&quot;
        ]
      }
    }
  }
}
EOF

cat &gt; ca-csr.json &lt;&lt;EOF
{
    &quot;CN&quot;: &quot;etcd CA&quot;,
    &quot;key&quot;: {
        &quot;algo&quot;: &quot;rsa&quot;,
        &quot;size&quot;: 2048
    },
    &quot;names&quot;: [
        {
            &quot;C&quot;: &quot;CN&quot;,
            &quot;L&quot;: &quot;Beijing&quot;,
            &quot;ST&quot;: &quot;Beijing&quot;
        }
    ]
}
EOF

cfssl gencert -initca ca-csr.json | cfssljson -bare ca

## 为etcd生成证书
cat &gt; server-csr.json &lt;&lt;EOF
{
    &quot;CN&quot;: &quot;etcd&quot;,
    &quot;hosts&quot;: [
    &quot;192.168.204.133&quot;,
    &quot;192.168.204.134&quot;,
    &quot;192.168.204.135&quot;
    ],
    &quot;key&quot;: {
        &quot;algo&quot;: &quot;rsa&quot;,
        &quot;size&quot;: 2048
    },
    &quot;names&quot;: [
        {
            &quot;C&quot;: &quot;CN&quot;,
            &quot;L&quot;: &quot;BeiJing&quot;,
            &quot;ST&quot;: &quot;BeiJing&quot;
        }
    ]
}
EOF

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare server

## 下载etcd二进制包开始部署
mkdir -p  /opt/etcd/{bin,cfg,ssl}
cp  /root/k8s/etcd-cert/{ca,server,server-key}.pem  /opt/etcd/ssl/
mkdir  /opt/src &amp;&amp; cd /opt/src
wget https://github.com/coreos/etcd/releases/download/v3.3.13/etcd-v3.3.13-linux-amd64.tar.gz
tar -xvf etcd-v3.3.13-linux-amd64.tar.gz
cp  etcd-v3.3.13-linux-amd64/etcd*  /opt/etcd/bin/

## etcd启动配置文件
ETCD_NAME=&quot;etcd01&quot;
ETCD_IP=&quot;192.168.204.133&quot;
ETCD_CLUSTER=&quot;etcd02=https://192.168.204.134:2380,etcd03=https://192.168.204.135:2380&quot;

cat &lt;&lt;EOF &gt;/opt/etcd/cfg/etcd
#[Member]
ETCD_NAME=&quot;${ETCD_NAME}&quot;
ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;
ETCD_LISTEN_PEER_URLS=&quot;https://${ETCD_IP}:2380&quot;
ETCD_LISTEN_CLIENT_URLS=&quot;https://${ETCD_IP}:2379&quot;

#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://${ETCD_IP}:2380&quot;
ETCD_ADVERTISE_CLIENT_URLS=&quot;https://${ETCD_IP}:2379&quot;
ETCD_INITIAL_CLUSTER=&quot;etcd01=https://${ETCD_IP}:2380,${ETCD_CLUSTER}&quot;
ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;
ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;
EOF

## 生成systemd启动文件
cat &lt;&lt;EOF &gt;/usr/lib/systemd/system/etcd.service
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
EnvironmentFile=${WORK_DIR}/cfg/etcd
ExecStart=${WORK_DIR}/bin/etcd \
--name=\${ETCD_NAME} \
--data-dir=\${ETCD_DATA_DIR} \
--listen-peer-urls=\${ETCD_LISTEN_PEER_URLS} \
--listen-client-urls=\${ETCD_LISTEN_CLIENT_URLS},http://127.0.0.1:2379 \
--advertise-client-urls=\${ETCD_ADVERTISE_CLIENT_URLS} \
--initial-advertise-peer-urls=\${ETCD_INITIAL_ADVERTISE_PEER_URLS} \
--initial-cluster=\${ETCD_INITIAL_CLUSTER} \
--initial-cluster-token=\${ETCD_INITIAL_CLUSTER_TOKEN} \
--initial-cluster-state=new \
--cert-file=${WORK_DIR}/ssl/server.pem \
--key-file=${WORK_DIR}/ssl/server-key.pem \
--peer-cert-file=${WORK_DIR}/ssl/server.pem \
--peer-key-file=${WORK_DIR}/ssl/server-key.pem \
--trusted-ca-file=${WORK_DIR}/ssl/ca.pem \
--peer-trusted-ca-file=${WORK_DIR}/ssl/ca.pem
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

## 启动
systemctl daemon-reload
systemctl enable etcd
systemctl restart etcd
</code></pre>
<h3 id="node节点部署flannel">node节点部署flannel</h3>
<pre><code>

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Yum部署k8s]]></title>
        <id>https://yes5144.github.io//post/k8s-bu-shu</id>
        <link href="https://yes5144.github.io//post/k8s-bu-shu">
        </link>
        <updated>2019-08-18T07:47:13.000Z</updated>
        <content type="html"><![CDATA[<h2 id="k8s安装入门">k8s安装入门</h2>
<pre><code>### 参考链接：https://www.cnblogs.com/sxdcgaq8080/p/10621437.html

## 准备工作
192.168.204.133  k8s-master01
192.168.204.134  k8s-node01
192.168.204.135  k8s-node02

### 分别设置主机名
hostnamectl --static set-hostname  k8s-master01
hostnamectl --static set-hostname  k8s-node01
hostnamectl --static set-hostname  k8s-node02


### 修改/etc/hosts

cat &gt;&gt;/etc/hosts &lt;&lt;EOF
192.168.204.133    k8s-master01
192.168.204.133    etcd
192.168.204.133    registry
192.168.204.134    k8s-node01
192.168.204.135    k8s-node02
EOF

### 关闭防火墙，SELinux
systemctl stop firewalld.service
systemctl disable firewalld.service

### 再使用查看命令查看，如果是如下效果，说明成功
firewall-cmd --state

### ntpdate
echo '*/10 * * * * root ntpdate  ntp2.aliyun.com' &gt;&gt; /etc/crontab
</code></pre>
<h3 id="一-主节点master01">一、主节点master01</h3>
<pre><code>主节点需要安装

etcd 存储数据中心

flannel k8s的一种网络方案

kubernetes 【包含：kube-api-server  controllerManager   Scheduler 】

## 1,etcd的安装
### 1.1 命令安装
yum install -y etcd
### 1.2 配置文件
cd  /etc/etcd
cp  etcd.conf  etcd.conf.default
cat etcd.conf
ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;
ETCD_LISTEN_CLIENT_URLS=&quot;http://0.0.0.0:2379,http://0.0.0.0:4001&quot;
ETCD_NAME=&quot;master&quot;
ETCD_ADVERTISE_CLIENT_URLS=&quot;http://etcd:2379,http://etcd:4001&quot;


### 1.3 启动etcd服务并验证
systemctl start etcd
systemctl enable etcd
systemctl status etcd

#### 检查健康状况
etcdctl -C http://etcd:2379 cluster-health
etcdctl -C http://etcd:4001 cluster-health

## 2,flannel的安装
### 2.1 安装命令


### 2.2 配置文件 /etc/sysconfig/flanneld
FLANNEL_ETCD_ENDPOINTS=&quot;http://etcd:2379&quot;
FLANNEL_ETCD_PREFIX=&quot;/atomic.io/network&quot;

### 2.3 配置etcd中关于flannel的key
etcdctl mk /atomic.io/network/config '{ &quot;Network&quot;: &quot;10.0.0.0/16&quot; }'

### 2.4 启动flannel服务，并设置开机自启
systemctl start flanneld.service
systemctl status flanneld.service
systemctl enable flanneld.service

## 3,安装kubernetes
### 3.1 安装命令
yum install kubernetes

### 3.2安装后，需要修改配置
### 配置修改是为了下面这些需要运行的组件
kube-api-server
kuber-scheduler
kube-controller-manager

vim /etc/kubernetes/apiserver

vim /etc/kubernetes/config


### 3.3 分别启动三个组件服务，并且设置为自启动
systemctl start kube-apiserver.service
systemctl start kube-controller-manager.service
systemctl start kube-scheduler.service


systemctl enable kube-apiserver.service
systemctl enable kube-controller-manager.service
systemctl enable kube-scheduler.service


## 4,安装docker
### 4.1 安装docker命令：
yum install  -y docker 
### 4.2 启动docker服务命令：
service docker start
### 4.3 docker加入自启动服务命令：
chkconfig docker on

</code></pre>
<h3 id="二-子节点安装">二、子节点安装</h3>
<pre><code>

## 1, 安装etcd


## 2, 安装flannel


## 3, 安装kubernetes


## 4, 安装docker


</code></pre>
<h3 id="三验证集群状态">三.验证集群状态</h3>
<pre><code>1.master节点执行命令，查看端点信息
kubectl get endpoints

2.master节点执行命令，查看集群信息
kubectl cluster-info

3.master节点执行命令，获取节点信息
kubectl get nodes


## 部署app
kubectl run kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1 --port=8080
kubectl run redis --image=docker.io/redis --port=6379

export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{&quot;\n&quot;}}{{end}}')
echo Name of the Pod: $POD_NAME

curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy/


kubectl describe pods

kubectl  logs $POD_NAME

kubectl exec $POD_NAME env

kubectl exec -it $POD_NAME bash

kubectl get deployments

kubectl scale deployments/kubernetes-bootcamp --replicas=4

kubectl get pods -o wide

kubectl describe deployments/kubernetes-bootcamp

</code></pre>
<h3 id="其他">其他</h3>
<pre><code>######################
## 配置k8s repo
cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
setenforce 0


yum install -y kubelet kubeadm kubectl
systemctl enable kubelet &amp;&amp; systemctl start kubelet

## 配置docker-ce repo
## 
# step 1: 安装必要的一些系统工具
sudo yum install -y yum-utils device-mapper-persistent-data lvm2
# Step 2: 添加软件源信息
sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
# Step 3: 更新并安装 Docker-CE
sudo yum makecache fast
sudo yum -y install docker-ce
# Step 4: 开启Docker服务
sudo service docker start

注意：其他注意事项在下面的注释中
# 官方软件源默认启用了最新的软件，您可以通过编辑软件源的方式获取各个版本的软件包。例如官方并没有将测试版本的软件源置为可用，你可以通过以下方式开启。同理可以开启各种测试版本等。
# vim /etc/yum.repos.d/docker-ce.repo
#   将 [docker-ce-test] 下方的 enabled=0 修改为 enabled=1
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[kubeadm部署k8s]]></title>
        <id>https://yes5144.github.io//post/kubeadm-bu-shu-k8s</id>
        <link href="https://yes5144.github.io//post/kubeadm-bu-shu-k8s">
        </link>
        <updated>2019-08-18T00:49:21.000Z</updated>
        <content type="html"><![CDATA[<h3 id=""></h3>
<pre><code>## https://www.bookstack.cn/read/kubernetes-handbook/SUMMARY.md
## https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/


</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mysql-主从复制（Docker）]]></title>
        <id>https://yes5144.github.io//post/mysql-zhu-cong</id>
        <link href="https://yes5144.github.io//post/mysql-zhu-cong">
        </link>
        <updated>2019-08-11T03:39:01.000Z</updated>
        <content type="html"><![CDATA[<h3 id="mariadbmysql的主从复制部署docker">MariaDB/Mysql的主从复制部署(Docker)</h3>
<pre><code>### 原文链接：https://blog.csdn.net/clearlxj/article/details/88313033

### 注意文中有一个笔误： /home/lxj/hedisql  /home/lxj/heidisql/

## 修改master_my.cnf，在 [mysqld] 节点下添加

[mysqld]
server-id=1
log_bin=master-bin
binlog-ignore-db=mysql
binlog-ignore-db=information_schema
binlog-ignore-db=performance_schema
binlog-ignore-db=test
innodb_flush_log_at_trx_commit=1
binlog_format=mixed


## 修改slave1_my.cnf，在 [mysqld] 节点下添加

[mysqld]
server-id=2
relay-log-index=slave-relay-bin.index
relay-log=slave-relay-bin
relay_log_recovery=1

</code></pre>
]]></content>
    </entry>
</feed>